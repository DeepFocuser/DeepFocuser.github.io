[ { "title": "DETR set prediction loss", "url": "/posts/detr/", "categories": "AI, 컴퓨터 비전", "tags": "DETR", "date": "2021-11-23 20:00:00 +0900", "snippet": " DETR 논문 바로가기DETR 논문을 읽었다. ‘vision 문제중 하나인 object detector를 Transformer로 풀었다 NMS도 필요없다. 또 panoptic segmentation 에까지 확장 가능하다.’ 라는 내용인데, 읽으면서 몇가지 의문이 들었다. 구현을 염두해둔 의문(positional encoding은 어떻게 코드로 적용할까나? 번역, 챗봇과 다른 vision문제에서 Transformer는 Attention map을 어떻게 그릴까나?)과 DETR에서 가장 중요한 내용인 hungarian algorithm 이다. hungarian algorithm에 관련된 의문을 제외하곤 페이스북의 공식 DETR 깃허브의 코드들을 보면서 의문점을 해결했다. DETR Attention 참고 DETR positional encoding 참고따라서 본 글에서는 논문에서 Hungarian algorithm 내용이 언급된 3.1 Object detection set prediction loss 부분을 심층 탐구 한다.(시간이 흘러 나중에 이 부분을 다시보면 이해가 잘 안될 것 같아서…)Hungarian algorithm 알고리즘에 대한 글은 여기를 보시라.(사실 Hungarian algorithm 알고리즘을 정확히 알아야 아래의 내용을 제대로 이해할 수 있다.)3.1 Object detection set prediction lossTo find a bipartite matching between these two sets we search for a permutation of N elements with $\\sigma\\epsilon\\mathfrak{S}_N$ the lowest cost:\\[\\hat{\\sigma}=\\underset {\\sigma\\epsilon\\mathfrak{S}_N}{\\operatorname{arg min}}\\sum_{i}^N\\zeta_{match}(y_i,\\hat{y}_\\sigma(i))\\]where $\\zeta_{match}(y_i,\\hat{y}_\\sigma(i))$ is pair-wise mathcing cost between ground truth $y_i$ and a prediction with index $\\sigma(i)$ This optimal assignment is computed efficiently with the Hungarian algorithm정리하자면, predicted object 와 ground truth간의 Optimal Bipartite Matching을 찾기 위해, 가장 낮은 비용을 갖는 N 요소들의 순서($\\sigma\\epsilon\\mathfrak{S}_{N}$)를 찾는 것이 위 식의 목적이고, 그 순서($\\hat{\\sigma}$)를 Hungarian algorithm을 사용해서 찾겠다는 말이다.참고 - 식의 의미?ground truth와 prediction이 매칭되었을 때 비용이 낮아진다. 그 때의 index(arg)을 구하는 것이다. 식을 보면 일단 최소 비용을 찾고(min) 이 비용에 대한 index(arg)를 찾는다.$\\sigma\\epsilon\\mathfrak{S}_{N}$ 는 $\\sigma$(index) 가 $\\mathfrak{S}_N$(N 요소들의 순서 집합)의 원소라는 얘기이다.그 외 참고 scipy Hungarian algorithm 문서 참고 MathJax 작성 참고" }, { "title": "Hungarian algorithm", "url": "/posts/hungarian-algorithm/", "categories": "알고리즘, 코드 조각", "tags": "Hungarian algorithm, Optimal Bipartite Matching", "date": "2021-11-22 20:00:00 +0900", "snippet": "DETR 논문을 읽기에 앞서 Hungarian algorithm 알고리즘에 대한 이해가 필요하여 정리해본다.Hungarian algorithm 알고리즘 코드 구현은 파이썬에서 “from scipy.optimize import linear_sum_assignment” 로 사용할 수 있다. 자세한 사용 방법은 여기를 보면 된다.Hungarian algorithmHungarian method는 다항 시간에 할당 문제를 해결하는 방법인 동시에 later primal–dual methods 예측(???)하는 조합 최적화 알고리즘이다.(예전에 배웠던것 같은데 기억이…) 이 알고리즘은 Harold Kuhn이 1955년에 개발했다. Hungarian method 라는 이름이 붙은 이유는 이 알고리즘이 두명의 헝가리 수학자 Dénes Kőnig와 Jenő Egerváry의 초기 연구에 기반을 두고 있기 때문이라고 한다.python 코드 구현참고 scipy Hungarian algorithm 문서 참고" }, { "title": "Maximum Bipartite Matching", "url": "/posts/maximum-bipartite-matching/", "categories": "알고리즘, 코드 조각", "tags": "Maximum Bipartite Matching", "date": "2021-11-05 20:00:00 +0900", "snippet": "DETR 논문을 읽기에 앞서 Maximum Bipartite Matching 알고리즘에 대한 이해가 필요하여 정리해본다. 사실 Optimal Bipartite Matching-Hungarian algorithm에 대한 내용이 필요하다.DETR을 이해하는데 있어 Maximum Bipartite Matching이 필요한 것은 아니지만 남겨놓자.Maximum Bipartite Matching 알고리즘 코드 구현은 DFS 코드를 기반으로 한다.Maximum Bipartite Matching(이분 매칭)두 개의 정점 그룹이 존재할 때 모든 간선(경로)의 용량이 1이면서 양쪽 정점이 서로 다른 그룹에 속하는 그래프를 이분 그래프(Bipartite Graph)라고 한다. 예를 들어, 한쪽 그룹은 X 그룹, 다른 한쪽 그룹은 Y 그룹이라고 할 때 모든 경로의 방향이 X-&amp;gt;Y인 그래프의 최대 유량을 구하는 것이 Bipartite Matching(이분 매칭)이다.이분 매칭을 통해 구하고자 하는 것은 최대 매칭 수이다. 매칭을 한다는 것은 어떤 정점이 그것이 가리키는 위치의 다른 정점을 점유한 상태를 말하며각 정점은 한 개씩만 점유 가능하고 여러개의 정점을 점유할 수 없다.python 코드 구현graph = { &#39;A&#39;: [1, 2], # A &#39;B&#39;: [1], # B &#39;C&#39;: [2, 3], # C &#39;D&#39;: [4, 5], # D &#39;E&#39;: [3] # E}# for문 이용한 dfs 코드로도 시도했으나 실패def dfs_recursive(departure_node): # dfs_recursive(result[node])에 or 앞에 오는 경우 # if start_node == &quot;NONE&quot;: # return False for destination_node in graph[departure_node]: # 이미 처리한 정점은 고려하지 않음 - 재귀시에만 값 유지 if visited[destination_node]: continue visited[destination_node] = True &#39;&#39;&#39; # dfs_recursive(result[node])에 or 앞에 오는 경우에는 아래의 코드가 필요 if start_node == &quot;NONE&quot;: # return False &#39;&#39;&#39; #if dfs_recursive(result[node]) or result[node] == &quot;NONE&quot;: # dfs_recursive(result[destination_node] 는 이전 노드를 다시 매칭하기 위함 if result[destination_node] == &quot;NONE&quot; or dfs_recursive(result[destination_node]): # 앞에 것이 True면 바로 if문 안으로 들어간다. result[destination_node] = departure_node # 매칭 return True return Falseif __name__ == &quot;__main__&quot;: &#39;&#39;&#39; 구현하기전 생각해볼 것? 필요한 변수 1. 시작점에서 목적지점까지 한 사이클을 돌 때, 방문했는지 안했는지 판단할 변수가 필요하다. --&amp;gt; visited 2. 목점지점 입장에서 시작점을 기록할 변수가 필요하다 --&amp;gt; result 좀더 빠른 속도를 위해 모든 변수 전부다 dictionary로 !!! &#39;&#39;&#39; graph_length = len(graph) result = { vertex_destination_node : vertex_departure_node for vertex_destination_node, vertex_departure_node in zip(range(1, graph_length+1), [&quot;NONE&quot;]*graph_length) } &#39;&#39;&#39; result print {1: &#39;NONE&#39;, 2: &#39;NONE&#39;, 3: &#39;NONE&#39;, 4: &#39;NONE&#39;, 5: &#39;NONE&#39;} &#39;&#39;&#39; for _, departure_node in enumerate(graph.keys(), start=1): # [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;] # visited는 시작 노드마다 각각 적용되어야 함 - for문 한 사이클 돌고 초기화 -&amp;gt; 시작노드가 n개 이므로 visited = {vertex_destination_node: judgment for vertex_destination_node, judgment in zip(range(1, graph_length+1), [False]*graph_length)} &#39;&#39;&#39; visitied print {1: False, 2: False, 3: False, 4: False, 5: False} &#39;&#39;&#39; dfs_recursive(departure_node) all_length=len(result.values()) matching_number = all_length - list(result.values()).count(&quot;NONE&quot;) print(f&quot;최대 매칭 : {matching_number}&quot;) for dest, dep in sorted(result.items(), key=lambda x: x[-1]): if dep != &quot;NONE&quot;: print(f&quot;{dep} : {dest}&quot;) &#39;&#39;&#39; print 최대매칭 : 4 A : 2 B : 1 C : 3 D : 4 &#39;&#39;&#39;" }, { "title": "BFS DFS", "url": "/posts/bfsdfs/", "categories": "알고리즘, 코드 조각", "tags": "BFS, DFS", "date": "2021-11-05 20:00:00 +0900", "snippet": "BFS : 넓이 우선 탐색 / for문을 이용한 방식(1가지)DFS : 깊이 우선 탐색 / for문과 재귀를 이용한 방식(2가지)python 코드 구현from collections import dequegraph = { &#39;A&#39;: [&#39;B&#39;], &#39;B&#39;: [&#39;A&#39;, &#39;C&#39;, &#39;H&#39;], &#39;C&#39;: [&#39;B&#39;, &#39;D&#39;], &#39;D&#39;: [&#39;C&#39;, &#39;E&#39;, &#39;G&#39;], &#39;E&#39;: [&#39;D&#39;, &#39;F&#39;], &#39;F&#39;: [&#39;E&#39;], &#39;G&#39;: [&#39;D&#39;], &#39;H&#39;: [&#39;B&#39;, &#39;I&#39;, &#39;J&#39;, &#39;M&#39;], &#39;I&#39;: [&#39;H&#39;], &#39;J&#39;: [&#39;H&#39;, &#39;K&#39;], &#39;K&#39;: [&#39;J&#39;, &#39;L&#39;], &#39;L&#39;: [&#39;K&#39;], &#39;M&#39;: [&#39;H&#39;]}path_bfs = []# Breadth First Search using queue - 재귀 불가def bfs(graph, start_node, end_node): start_node = start_node.upper() end_node = end_node.upper() visit = {} queue = deque() queue.append(start_node) while queue: node = queue.popleft() # 첫번째 요소부터 pop if node not in visit: visit[node] = True if node == end_node: # visit.keys()는 mutable 이므로 복사를 해준다 path_bfs.append(list(visit.keys())) # 왼쪽부터 오른쪽으로 검사 queue.extend(graph[node]) return list(visit.keys())path_dfs = []# depth first search using stackdef dfs(graph, start_node, end_node): start_node = start_node.upper() end_node = end_node.upper() visit = {} queue = deque() queue.append(start_node) while queue: node = queue.pop() # 마지막 요소부터 pop if node not in visit: visit[node] = True if node == end_node: # visit.keys()는 mutable 이므로 복사를 해준다 path_dfs.append(list(visit.keys())) # why reverse ? 왼쪽에서 오른쪽으로 검사 queue.extend(reversed(graph[node])) #queue.extend(graph[node]) return list(visit.keys())# dfs recursive!path_dfs_recur = []def dfs_recursive(graph, start_node, end_node, visit=dict()): start_node = start_node.upper() end_node = end_node.upper() visit[start_node] = True if start_node == end_node: # visit.keys()는 mutable 이므로 복사를 해준다 path_dfs_recur.append(list(visit.keys())) for node in graph[start_node]: if node not in visit: dfs_recursive(graph, node, end_node, visit) return list(visit.keys())if __name__ == &quot;__main__&quot;: print(&quot;bfs :&quot;,bfs(graph, start_node=&#39;A&#39;, end_node=&quot;D&quot;)) print(&quot;bfs path :&quot;, path_bfs) print(&quot;dfs :&quot;,dfs(graph, start_node=&#39;A&#39;, end_node=&quot;D&quot;)) print(&quot;dfs path :&quot;, path_bfs) print(&quot;dfs_recursive :&quot;,dfs_recursive(graph, start_node=&#39;A&#39;, end_node=&quot;D&quot;)) print(&quot;dfs_recursive path :&quot;, path_bfs) &#39;&#39;&#39; print bfs : [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;H&#39;, &#39;D&#39;, &#39;I&#39;, &#39;J&#39;, &#39;M&#39;, &#39;E&#39;, &#39;G&#39;, &#39;K&#39;, &#39;F&#39;, &#39;L&#39;] bfs path : [[&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;H&#39;, &#39;D&#39;]] dfs : [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;, &#39;F&#39;, &#39;G&#39;, &#39;H&#39;, &#39;I&#39;, &#39;J&#39;, &#39;K&#39;, &#39;L&#39;, &#39;M&#39;] dfs path : [[&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;H&#39;, &#39;D&#39;]] dfs_recursive : [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;, &#39;F&#39;, &#39;G&#39;, &#39;H&#39;, &#39;I&#39;, &#39;J&#39;, &#39;K&#39;, &#39;L&#39;, &#39;M&#39;] dfs_recursive path : [[&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;H&#39;, &#39;D&#39;]] &#39;&#39;&#39;" }, { "title": "Beam Search Decoder", "url": "/posts/beamsearch/", "categories": "AI, 자연어 처리", "tags": "Beam Search", "date": "2021-10-07 20:00:00 +0900", "snippet": " https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/ 를 참고Beam Search DecoderBeam Search는 Greedy Search 알고리즘(k=1)을 확장한 것이며, output sequences 의 리스트를 반환한다. beam search는 가능한 모든 단계(경로 를 탐색하고, k개를 유지한다. k는 사용자가 설정하는 hyper-parameter이며 beam의 수 또는 sequence 확률을 통한 병렬 탐색을 제어한다. 기계번역 작업에서는 보통 k=5 or k=10을 사용 한다. k가 크면 여러 candidate sequence들이 target sequence와 매칭될 가능성이 더 높아지기 때문에 좋은 성능을 보이나, 이는 결과적으로는 decoding 속도를 감소시킨다.(성능 vs 속도 trade-off 관계)Beam Search Algorithm주어진 확률 sequence 와 beam width parameter k에 대해 beam search를 수행하는 함수를 정의할 수 있다. 각 단계에서, 각 candidate sequence들은 가능한 다음 경로들로 확장된다. 각 candidate step 은 확률들을 곱하여 점수를 계산한다. 확률이 높은 k개의 후보만 선택되고 나머지는 제거된다. 이 과정은 sequence의 끝에 다다를때까지 반복된다. search process는 아래의 경우에 멈춘다. end-of-sequence token에 도달할 때 a maximum length에 도달할 때 threshold likelihood에 도달할 때 # beam searchimport numpy as npdef beam_search_decoder(data, k): sequences_index = [[list(), 0.0]] for row in data: all_candidates = list() # 가능한 다음 경로로 확장하기 for i in range(len(sequences_index)): seq, score = sequences_index[i] for j in range(len(row)): &#39;&#39;&#39; np.log에 -를 붙여서 최소화 문제로 바꿈. 가장 이상적인 경우는 score = 0 가장 안좋은 경우 score = 무한대 &#39;&#39;&#39; candidate = [seq + [j], score - np.log(row[j]+1e-7)] all_candidates.append(candidate) # score에 따라 오름차순 정렬 ordered = sorted(all_candidates, key=lambda tup:tup[1]) # best k개 뽑기 sequences_index = ordered[:k] return sequences_index# 5개 단어의 어휘에 대해 10개 단어의 시퀀스를 정의probability_sequence = [[0.1, 0.2, 0.3, 0.4, 0.5], [0.5, 0.4, 0.3, 0.2, 0.1], [0.1, 0.2, 0.3, 0.4, 0.5], [0.5, 0.4, 0.3, 0.2, 0.1], [0.1, 0.2, 0.3, 0.4, 0.5], [0.5, 0.4, 0.3, 0.2, 0.1], [0.1, 0.2, 0.3, 0.4, 0.5], [0.5, 0.4, 0.3, 0.2, 0.1], [0.1, 0.2, 0.3, 0.4, 0.5], [0.5, 0.4, 0.3, 0.2, 0.1]]result = np.array(probability_sequence)result = beam_search_decoder(result, 3)# 결과 출력for sequence_index in result: print(sequence_index)&#39;&#39;&#39;결과[[4, 0, 4, 0, 4, 0, 4, 0, 4, 0], 6.9314698055996535][[4, 0, 4, 0, 4, 0, 4, 0, 4, 1], 7.154613306913874][[4, 0, 4, 0, 4, 0, 4, 0, 3, 0], 7.154613306913874]&#39;&#39;&#39;이제 출력으로 나온 sequence_index 를 단어 사전을 이용해 단어로 바꿔주면 decoding이 완성된다." }, { "title": "Attention Is ALL You Need", "url": "/posts/transformer/", "categories": "AI, 자연어 처리", "tags": "Transformer", "date": "2021-10-06 20:00:00 +0900", "snippet": " Attention Is ALL You Need 논문 바로가기대학원에서 공부할 당시 자연어 처리 맛만 보고, 회사 다니고 나서는 자연어 처리쪽 업무 경험은 전무하다. 앞으로도 특별한 일이 없는 한 딱히 있을 것 같진 않지만… transformer가 자연어처리 외에 다른 분야에도 사용되는 경우도 많다고 하니( ex) DETR ), 리뷰를 해보자. 본 논문은 transformer Model 구조 파악이 목적이다. 본인은 컴퓨터 비전 관련한 일을 하기 때문에…자세히 리뷰하든, 간단히 리뷰하든 나중에 결국은 잊어먹더라… 최대한 핵심만 남겨놓자.AbstractRnn, Convolution Layer을 사용하지 않은 Encoder, Decoder 구조 즉, transformer를 제안한다. 영어-독어 번역, 영어-프랑스어 번역에서 가장 좋은 결과를 냈고, 다른 작업(English Constituency parsing)에서도 성공적으로 적용이 가능했다고 한다.병렬화 학습 가능하고, 학습 시간 적게 걸리고, BLUE 지표에서 엄청 좋았다는등 자랑도 한다.IntroductionRecurrent neural network 기반의 모델이 최신을 이끌어왔고, 적용범위를 넓히기 위한 노력들을 계속해왔다.Recurrent model은 시간을 포함하는 특성때문에 병렬화를 어렵게 한다. 최근에는 factorization tricks 와 conditional computation 같은 기법으로 계산 효율성을 증가시켰다고 한다. conditional computation는 모델성능까지 증가시켰다고 한다. 그러나 여전히 시간을 계산한다는 근본적인 제약이 남아 있다.Attention 메커니즘은 다양한 작업에서 강력한 시퀀스 모델링이나 변환 모델에서 필수적인 부분이고, 입력 또는 출력의 시퀀스의 거리에 관계 없이 종속성 모델링이 가능하게 한다. 모든 경우는 아니지만, 몇몇 경우에는 Attention 메커니즘이 Recurrent network와 함께 사용된다고 한다.(seq2seq with attention 같은 것을 말하는 듯 하다.)BackgroundByteNet 나 ConvS2S는 두 임의의 입력 또는 출력 위치의 신호를 연관시키는데 필요한 operation의 수가 위치간의 거리에 따라 증가한다. 이러한 점은 먼 위치사이 에서의 의존도를 학습하기 어렵게 만든다고 한다. 그런데 transformer는 operation의 수가 상수라고 한다. (Muiti-Head Attention 때문이라고 한다.) 정확히 어떤의미인지는 와닿지 않으나, transformer가 입,출력간의 의존도를 학습하는데 있어서 ByteNet, ConvS2S와 같은 모델들보다 계산이 적게 들어간다는 의미인듯 하다.Self-attention은 시퀀스의 표현을 계산하기 위해 단일 시퀀스의 다른 위치들을 연관시키는 Attention 메커니즘이다.End-to-End memory 네트워크는 recurrent 어텐션 메커니즘에 기반한다.Transformer는 오로지 Self-attention에 의존한 첫번째 변환 모델이다.Model Architecture Transformer 구조 stacked self-attention point-wise fully connected layers Encoder and Decoder Stacks Encoder encoder는 6개의 동일한 층을 쌓은 구조 각 층은 2개의 sub-layer를 가지고 있음. 첫번째 층 : multi-head self-attention mechanism 두번째 층 : position-wise fully connected feed-forward network residual connection 사용 / layer normalization 사용 각 sub-layer의 output : LayerNorm(x+Sublayer(x)) residual connection을 사용하기 위해 모든 sub-layer, embedding layers는 512 차원의 output 을 생성 Decoder decoder도 encoder와 같이 6개의 동일한 층을 쌓은 구조 encoder의 sub-layer 2개에 + 1개의 sub-layer를 더 넣음(multi-head attention) encoder stack의 출력에 대해 multi-head-attention을 수행한다. residual connection 사용 / layer normalization 사용 modified self-attention sub-layer 현재 위치가 다음 위치에 주목하는 것을 방지 하기 위한 장치 이 masking은 output embeddings들이 한 위치씩 offset되어 있다는 사실과 결합되어 i위치에 대한 예측이 반드시 위치 i보다 작은 위치에서 알려진 출력에만 의존할 수 있도록 한다. 무슨 말인지 와닿지 않는다.(1) - 해결 Transformer는 문장 행렬로 입력을 한꺼번에 받으므로 현재 시점의 단어를 예측하고자 할 때, 입력 문장 행렬로부터 미래 시점의 단어 까지도 참고할 수 있는 현상이 발생한다. 이 문제를 해결하기 위해 Transformer의 decoder는 현재 시점의 예측에서 현재 시점보다 미래에 있는 단어를 참고하지 못하도록 마스크를 씌워준다는 얘기이다. 자세한 설명은 여기를 보면 될 것 같다. 코드와 설명 주석이 있는 링크 위 코드에서 생성한 mask 그림 encoder와 decoder의 입력 문장에 토큰이 있는 경우 attention에서 제외해주는 mask도 같이 적용한 결과이다. 토큰의 경우에는 실질적인 의미를 가진 단어가 아니므로, Transformer에서는 Key의 경우에 토큰이 존재한다면 이에 대해서는 계산을 제외하도록 마스킹(Masking)을 해준다. Attention attention 함수는 query와 key-value 쌍을 output으로 맵핑 하는 것으로 설명될 수 있다. 여기서 query, key, values 그리고 output은 모두 다 vector이다. 출력은 values의 가중치 합으로 계산되며. 여기서 각 value에 할당된 가중치들은 해당 key를 가진 query의 호환성 함수에 의해 계산된다.-&amp;gt;(weight는 query, key로 만들고, 그 만들어진 weight를 value와 계산한다.) Scaled Dot-Product Attention 입력은 $queries,keys : d_{k} 차원, values : d_{v} 차원으로 구성 된다.$ 모든 키를 사용하여 쿼리의 내적을 계산하고, $\\sqrt{d_k}$ 로 나눈다. 그리고 values에 대한 가중치를 얻기 위해 softmax 함수를 적용한다. 실제로는, queries 세트에 대한 attention 함수를 동시에 계산하여 행렬 Q로 묶는다. 키와 값도 행렬 K와 V로 묶입니다. 출력 행렬을 다음과 같이 계산한다.\\(Attention(Q, K, V) = softmax({QK^T \\over \\sqrt{d_k}})\\) 가장 많이 사용하는 attention 함수들로는 additive attention 함수, dot-product 함수가 있다. dot-product 함수는 scaling factor $1 \\over \\sqrt{d_k}$가 있다는 점 외에는 본 논문에서 사용한 알고리즘과 동일하다. 두 알고리즘은 비슷하지만, dot-product attention 함수가 실전에서 더 빠르고 공간 효율적이라고 한다. Multi-Head Attention $d_{model}-차원$의 keys, values, queries를 사용하여 single attention function 을 수행하는 대신, $d_{k}, d_{k}, d_{v}$차원에 대해 학습된 서로 다른 선형 프로젝션을 사용하여 queries, keys, values 를 h번 선형으로 투영하는 것이 좋다는 것을 발견했다. 이렇게 하면 $d_{v}-dimensional$ output values를 산출하는 attention function을 병렬적으로 수행할 수 있다. 이 값들은 concat 되고 한번더 투영되어, 최종적인 값을 결과로 뽑아낸다.(Figure 2 보라)Multi-head attention을 통해 모델이 서로 다른 위치에서 서로 다른 표현 하위 공간의 정보에 공동으로 주의를 기울일 수 있다. 하나의 attention head로, 평균을 내는 것은 공동으로 주의를 기울이는 것을 억제한다. \\(MultiHead(Q, K, V) = Concat(Head_{1}, ..., head_{h})W^O\\)\\(where Head_{i} = Attention(QW_{i}^Q, KW_{i}^K, VW_{i}^V)\\) 여기서 projections 들은 파라미터 행렬들이다. \\(W_{i}^Q \\varepsilon R^{d_{model} X d_{k}}, W_{i}^K \\varepsilon R^{d_{model} X d_{k}}, W_{i}^V \\varepsilon R^{d_{model} X d_{v}} and W^O \\varepsilon R^{hd_{v} X d_{model}}\\) h = 8 인 병렬 attention layer, heads를 사용했으며, 각각 $ d_{k} = d_{v} = {d_{model} \\over h} = 64$ 이다. Applications of Attention in out Model encoder-decoder attention layer안에서, queries 는 이전 decoder layer 에서, memory keys, values 는 encoder의 출력에서 온다. 이는 decoder의 모든 위치가 입력 sequence의 모든 위치들에 주의를 기울일 수 있게 하는 것을 가능하게 한다.seq2seq 모델안의 encoder-decoder attention 메커니즘을 흉내냈다. encoder는 self-attention layers 들을 포함한다. self-attention layer에서 모든 keys, values, queries 는 같은 장소에서 오고, 이 경우에 encoder의 이전 layer의 output이다.encoder의 각 위치들은 encoder의 이전 layer의 모든 위치들에 주의를 기울일수 있다. 비슷하게, decoder의 self-attention layer는 decoder의 각 위치가 decoder의 해당 위치까지 그리고 그 위치를 포함하는 모든 위치에 주의를 기울일 수 있도록 한다. 자동 회귀 속성을 유지하려면 decoder에서 왼쪽으로의 정보 흐름을 방지해야 한다. 잘못된 연결에 해당하는 softmax 입력의 모든 값을 마스킹(-∞로 설정)하여 scaled dot-product Attention 내부에서 이것을 구현한다. (Figure 2를 보라.) 무슨 말인지 와닿지 않는다.(2)-해결 무슨 말인지 와닿지 않는다.(1) 의 해결 내용과 같다. Position-wise Feed-Forward Networks attention sub-layers 외’에도 encoder 및 decoder의 각 계층에는 각 위치에 개별적이고 동일하게 적용되는 fully connected feed-forward network 가 포함된다. 이것은 사이에 ReLU 활성화가 있는 두 개의 선형 변환으로 구성된다.\\(FFN(x) = max(0, xW_{1} + b_{1})W_{2} + b_{2}\\)선형 변환은 다른 위치들에서 동일하지만, layer마다 다른 파라미터들을 사용한다. Embeddings and Softmax 다른 sequence 변환 모델과 비슷하게, 우리는 input tokens 과 output tokens를 $d_{model}$ 차원 벡터로 바꾸기 위해 학습된 embeddings 을 사용한다. 우리는 또한 decoder 출력을 예측된 next-token 확률로 바꾸기 위해 학습된 선형 변환과 softmax함수를 사용한다. 우리 모델에서는 2개의 embedding layers 와 pre-softmax(예측 softmax) 선형 변환 간에 같은 가중치 행렬을 공유한다. embedding layer에서는 $\\sqrt{d_{model}}$을 가중치에 곱한다. 코드를 봐야 알 것 같다.(3) - 해결 코드와 설명 주석이 있는 링크 https://nlp.seas.harvard.edu/2018/04/03/attention.html 에 Shared Embeddings 이란 제목으로 설명되어 있다. Positional Encoding transformer는 recurrence 와 convolution을 포함하고 있지 않기 때문에 모델이 시간 순서정보를 사용하게 하기 위해서, sequence의 토큰의 상대적 위치 또는 절대적 위치에 어떤 정보를 넣어줘야만 한다. 이를 위해서 encoder와 decoder stacks의 아랫부분의 input embeddings에 positional encoding이라는 것을 추가한다. embeddings과 positional encoding이 덧셈이 가능하게 하기 위해 positional encoding 은 embeddings과 같이 $d_{model}$ 차원을 가진다. 본 논문에서는 다른 주기를 가지는 sine, cosine 함수를 사용한다.\\[PE_{pos, 2i} = sin({position \\over 10000^{2i \\over d_{model}}})\\]\\[PE_{pos, 2i+1} = cos({position \\over 10000^{2i \\over d_{model}}})\\] 여기 내용을 이해가기가 쉽지 않다.(4) seq2seq 모델같은 경우는 RNN을 사용하므로, 입력 자체에 시간 속성이 부여되어 있다. 그런데 transformer같은 경우는 그런게 없다. 그래서 transformer의 encoder, decoder에 시간 속성을 부여하기 위해서 위의 sin, cos 함수를 더해주는 것이다. 코드와 설명 주석이 있는 링크 위 코드에서 생성한 Positional Encoding 그림 아래의 내용부터는 그렇게 중요하다고 생각되지 않는다. 따라서 무슨내용을 다뤘는지만 간단히 설명하고 넘어간다.Why Self-Attention왜 self-Attention을 사용했는지에 대해 말하고 있고, 몇가지 장점을 설명하고 있다.(계산량, 병렬화, long-range dependencies 학습)부수적인 이익으로 self-attention은 해석가능한 모델을 산출해낸다고 한다.Training Training Data and Batching 데이터셋에 대한 설명 Hardward and Schedule 어떤 GPU룰 썼고, 어떻게 학습을 했고, 어떤 hyperparameter를 사용했는지에 대한 설명 Optimizer 어떤 Optimizer를 사용했고, 학습률은 어떻게 설정했는지에 대한 설명 Regularization 학습할때 사용한 3가지 type의 regularization 설명 Results Machine Translation Model Variations English Constituency Parsing Conclusion번역 작업에서 Transformer은 RNN or CONV layer 기반의 아키텍쳐보다 훨씬 더 빠르게 학습이 가능했음. WMT2014 데이터셋 기반 영어-독어 번역, 영어-프랑스어 번역 작업에서 가장 좋은 결과를 성취할 수 있었다.Transformer 모델을 input-output 구조를 가지고 있는 문제들(images, audio, video)에 확장할 계획이다.Code논문을 리뷰하며 잘 이해가 되지 않는 부분들이 있었다.(표시해 놓음) 이제 코드를 구현하면서 내가 제대로 이해하지 못한 부분을 채워나가는 시간이 필요할 것 같다. 며칠에 걸쳐서 독일어-영어 번역기 Transformer 모델 구현을 완료했다. 역시 논문을 읽는 것과 구현 사이에는 엄청난 괴리가 있다. 수많은 사이트들을 참고했고, 하나하나 직접 구현했다. 그 결과는 여기 내 깃허브 저장소에 있다. 상세한 설명과 참고 자료등을 주석으로 달아놨으니 누군가에게 도움이 되길바란다." }, { "title": "CTC Loss의 이해", "url": "/posts/ctcloss/", "categories": "AI, 컴퓨터 비전", "tags": "CTC Loss", "date": "2021-10-02 20:00:00 +0900", "snippet": " https://towardsdatascience.com/intuitively-understanding-connectionist-temporal-classification-3797e43a86c 를 참고CTC Loss의 이해진행중 ~" } ]
